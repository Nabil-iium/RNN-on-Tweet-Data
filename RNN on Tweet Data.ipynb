{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"pVYNu9wMdkko","trusted":true},"outputs":[],"source":["#Importing necesarry libraries\n","import pandas as pd \n","import seaborn as sns\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from sklearn.model_selection import train_test_split\n","import tensorflow.compat.v2 as tf\n","import pandas as pd\n","import numpy as np \n","import warnings\n","warnings.filterwarnings('ignore')\n","import re\n","import string\n","import pickle"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"hycEYqiDlC8p","outputId":"217a0d84-a615-4c25-b37b-8814b2f76049","trusted":true},"outputs":[],"source":["#importing dataset\n","data = pd.read_csv(r'C:\\Users\\ASUS\\Documents\\Python\\data.csv', encoding='latin', names = ['polarity','id','date','query','user','text'])"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Uu3dZ9eeBZOU","trusted":true},"outputs":[],"source":["data = data.sample(frac=1)\n","data = data[:200000]"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"f1lt303exS3J","outputId":"d98c81bc-29ee-46e6-e028-e1692af6176f","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset shape: (200000, 6)\n"]}],"source":["print(\"Dataset shape:\", data.shape)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"MKn0VS_kdkkr","outputId":"7e4e692e-af3f-4605-9a0b-3e62bc7028ce","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>polarity</th>\n","      <th>id</th>\n","      <th>date</th>\n","      <th>query</th>\n","      <th>user</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1308994</th>\n","      <td>4</td>\n","      <td>2012954704</td>\n","      <td>Tue Jun 02 21:37:00 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>lindsayrcg</td>\n","      <td>thinking about laying down. pretty sure maggie...</td>\n","    </tr>\n","    <tr>\n","      <th>1156126</th>\n","      <td>4</td>\n","      <td>1979067708</td>\n","      <td>Sun May 31 01:30:24 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>bestnileyfans</td>\n","      <td>@JonasVideos hey, thanks for following us. Tha...</td>\n","    </tr>\n","    <tr>\n","      <th>1360524</th>\n","      <td>4</td>\n","      <td>2048719161</td>\n","      <td>Fri Jun 05 15:39:42 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>carillee</td>\n","      <td>Going to Lido's for dinner.</td>\n","    </tr>\n","    <tr>\n","      <th>541612</th>\n","      <td>0</td>\n","      <td>2200148421</td>\n","      <td>Tue Jun 16 18:30:17 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>dianaily0</td>\n","      <td>Going to romania. Pretty scared.</td>\n","    </tr>\n","    <tr>\n","      <th>301220</th>\n","      <td>0</td>\n","      <td>1998590560</td>\n","      <td>Mon Jun 01 18:48:11 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>marii85</td>\n","      <td>Can't sleep  testing twitter from my mobile</td>\n","    </tr>\n","    <tr>\n","      <th>505774</th>\n","      <td>0</td>\n","      <td>2188476369</td>\n","      <td>Mon Jun 15 21:58:16 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>FaithDani</td>\n","      <td>Just loves how @justinjeremy NEVER tweets back...</td>\n","    </tr>\n","    <tr>\n","      <th>1234363</th>\n","      <td>4</td>\n","      <td>1992375287</td>\n","      <td>Mon Jun 01 08:39:07 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>pusscat30</td>\n","      <td>is basking in the sun</td>\n","    </tr>\n","    <tr>\n","      <th>1191119</th>\n","      <td>4</td>\n","      <td>1983830281</td>\n","      <td>Sun May 31 13:55:10 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>redrobinrockn</td>\n","      <td>@VickiElam U should meet @StatiK99 cause he's ...</td>\n","    </tr>\n","    <tr>\n","      <th>898330</th>\n","      <td>4</td>\n","      <td>1693583396</td>\n","      <td>Sun May 03 23:02:01 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>karleexx</td>\n","      <td>400th tweet!!  yay</td>\n","    </tr>\n","    <tr>\n","      <th>1183352</th>\n","      <td>4</td>\n","      <td>1982415840</td>\n","      <td>Sun May 31 11:09:50 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>viridian</td>\n","      <td>Guess who finally has a new Blackberry?</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         polarity          id                          date     query  \\\n","1308994         4  2012954704  Tue Jun 02 21:37:00 PDT 2009  NO_QUERY   \n","1156126         4  1979067708  Sun May 31 01:30:24 PDT 2009  NO_QUERY   \n","1360524         4  2048719161  Fri Jun 05 15:39:42 PDT 2009  NO_QUERY   \n","541612          0  2200148421  Tue Jun 16 18:30:17 PDT 2009  NO_QUERY   \n","301220          0  1998590560  Mon Jun 01 18:48:11 PDT 2009  NO_QUERY   \n","505774          0  2188476369  Mon Jun 15 21:58:16 PDT 2009  NO_QUERY   \n","1234363         4  1992375287  Mon Jun 01 08:39:07 PDT 2009  NO_QUERY   \n","1191119         4  1983830281  Sun May 31 13:55:10 PDT 2009  NO_QUERY   \n","898330          4  1693583396  Sun May 03 23:02:01 PDT 2009  NO_QUERY   \n","1183352         4  1982415840  Sun May 31 11:09:50 PDT 2009  NO_QUERY   \n","\n","                  user                                               text  \n","1308994     lindsayrcg  thinking about laying down. pretty sure maggie...  \n","1156126  bestnileyfans  @JonasVideos hey, thanks for following us. Tha...  \n","1360524       carillee                       Going to Lido's for dinner.   \n","541612       dianaily0                  Going to romania. Pretty scared.   \n","301220         marii85        Can't sleep  testing twitter from my mobile  \n","505774       FaithDani  Just loves how @justinjeremy NEVER tweets back...  \n","1234363      pusscat30                             is basking in the sun   \n","1191119  redrobinrockn  @VickiElam U should meet @StatiK99 cause he's ...  \n","898330        karleexx                                 400th tweet!!  yay  \n","1183352       viridian          Guess who finally has a new Blackberry?    "]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["data.head(10)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"CEGPNsXRdkks","outputId":"d7a07409-b827-4c03-ee78-a87a2a2b03cf","trusted":true},"outputs":[{"data":{"text/plain":["array([4, 0], dtype=int64)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["data['polarity'].unique()"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"Tl4afas1W20w","outputId":"8e3d79af-1590-4c87-e2c1-aadc45e651e4","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>polarity</th>\n","      <th>id</th>\n","      <th>date</th>\n","      <th>query</th>\n","      <th>user</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1308994</th>\n","      <td>1</td>\n","      <td>2012954704</td>\n","      <td>Tue Jun 02 21:37:00 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>lindsayrcg</td>\n","      <td>thinking about laying down. pretty sure maggie...</td>\n","    </tr>\n","    <tr>\n","      <th>1156126</th>\n","      <td>1</td>\n","      <td>1979067708</td>\n","      <td>Sun May 31 01:30:24 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>bestnileyfans</td>\n","      <td>@JonasVideos hey, thanks for following us. Tha...</td>\n","    </tr>\n","    <tr>\n","      <th>1360524</th>\n","      <td>1</td>\n","      <td>2048719161</td>\n","      <td>Fri Jun 05 15:39:42 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>carillee</td>\n","      <td>Going to Lido's for dinner.</td>\n","    </tr>\n","    <tr>\n","      <th>541612</th>\n","      <td>0</td>\n","      <td>2200148421</td>\n","      <td>Tue Jun 16 18:30:17 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>dianaily0</td>\n","      <td>Going to romania. Pretty scared.</td>\n","    </tr>\n","    <tr>\n","      <th>301220</th>\n","      <td>0</td>\n","      <td>1998590560</td>\n","      <td>Mon Jun 01 18:48:11 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>marii85</td>\n","      <td>Can't sleep  testing twitter from my mobile</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         polarity          id                          date     query  \\\n","1308994         1  2012954704  Tue Jun 02 21:37:00 PDT 2009  NO_QUERY   \n","1156126         1  1979067708  Sun May 31 01:30:24 PDT 2009  NO_QUERY   \n","1360524         1  2048719161  Fri Jun 05 15:39:42 PDT 2009  NO_QUERY   \n","541612          0  2200148421  Tue Jun 16 18:30:17 PDT 2009  NO_QUERY   \n","301220          0  1998590560  Mon Jun 01 18:48:11 PDT 2009  NO_QUERY   \n","\n","                  user                                               text  \n","1308994     lindsayrcg  thinking about laying down. pretty sure maggie...  \n","1156126  bestnileyfans  @JonasVideos hey, thanks for following us. Tha...  \n","1360524       carillee                       Going to Lido's for dinner.   \n","541612       dianaily0                  Going to romania. Pretty scared.   \n","301220         marii85        Can't sleep  testing twitter from my mobile  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# Replacing the value 4 to 1.\n","data['polarity'] = data['polarity'].replace(4,1)\n","data.head()"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"evVpnAuDwo64","outputId":"a74f56e3-e55b-4c18-e854-32cbba487025","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>polarity</th>\n","      <th>id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>200000.000000</td>\n","      <td>2.000000e+05</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.502180</td>\n","      <td>1.997965e+09</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.499996</td>\n","      <td>1.934211e+08</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>1.467814e+09</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.000000</td>\n","      <td>1.956658e+09</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>1.000000</td>\n","      <td>2.001829e+09</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>1.000000</td>\n","      <td>2.176809e+09</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>1.000000</td>\n","      <td>2.329205e+09</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            polarity            id\n","count  200000.000000  2.000000e+05\n","mean        0.502180  1.997965e+09\n","std         0.499996  1.934211e+08\n","min         0.000000  1.467814e+09\n","25%         0.000000  1.956658e+09\n","50%         1.000000  2.001829e+09\n","75%         1.000000  2.176809e+09\n","max         1.000000  2.329205e+09"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["data.describe()"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"iTN9l7p2w_N0","outputId":"c03ca38a-2536-479a-9219-fcef71791510","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total length of the data is:         200000\n","No. of positve tagged sentences is:  100436\n","No. of negative tagged sentences is: 99564\n"]}],"source":["# checking positive and negative labeled tweets\n","positives = data['polarity'][data.polarity == 1 ]\n","negatives = data['polarity'][data.polarity == 0 ]\n","\n","print('Total length of the data is:         {}'.format(data.shape[0]))\n","print('No. of positve tagged sentences is:  {}'.format(len(positives)))\n","print('No. of negative tagged sentences is: {}'.format(len(negatives)))"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"FZCZYZKn00Iv","trusted":true},"outputs":[],"source":["def word_count(words):\n","    return len(words.split())"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"AudC8DVW07Cf","outputId":"65ebdbd3-ae34-4ebe-88b2-e96fa7c099fe","trusted":true},"outputs":[{"data":{"text/plain":["[('the', 64758),\n"," ('and', 37208),\n"," ('you', 29842),\n"," ('for', 26467),\n"," ('have', 17946),\n"," ('that', 16109),\n"," (\"i'm\", 15889),\n"," ('but', 15561),\n"," ('just', 15353),\n"," ('with', 14167),\n"," ('was', 12740),\n"," ('not', 12715),\n"," ('this', 11148),\n"," ('get', 10028),\n"," ('good', 9758),\n"," ('are', 9555),\n"," ('like', 9383),\n"," ('all', 9278),\n"," ('out', 8649),\n"," ('your', 8234)]"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["from collections import Counter\n","all_words = []\n","for line in list(data['text']):\n","    words = line.split()\n","    for word in words:\n","      if(len(word)>2):\n","        all_words.append(word.lower())\n","    \n","    \n","Counter(all_words).most_common(20)"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"VE6nqmh5dkku","trusted":true},"outputs":[],"source":["# dropping unnecesarry columns\n","data.drop(['date','query','user','word count'], axis=1, inplace=True)"]},{"cell_type":"code","execution_count":22,"metadata":{"trusted":true},"outputs":[],"source":["data.drop('id', axis=1, inplace=True)"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"XNSb32D9dkku","outputId":"dc5e8e0e-041d-4fdc-f94d-716c17fcaa52","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>polarity</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1308994</th>\n","      <td>1</td>\n","      <td>thinking about laying down. pretty sure maggie...</td>\n","    </tr>\n","    <tr>\n","      <th>1156126</th>\n","      <td>1</td>\n","      <td>@JonasVideos hey, thanks for following us. Tha...</td>\n","    </tr>\n","    <tr>\n","      <th>1360524</th>\n","      <td>1</td>\n","      <td>Going to Lido's for dinner.</td>\n","    </tr>\n","    <tr>\n","      <th>541612</th>\n","      <td>0</td>\n","      <td>Going to romania. Pretty scared.</td>\n","    </tr>\n","    <tr>\n","      <th>301220</th>\n","      <td>0</td>\n","      <td>Can't sleep  testing twitter from my mobile</td>\n","    </tr>\n","    <tr>\n","      <th>505774</th>\n","      <td>0</td>\n","      <td>Just loves how @justinjeremy NEVER tweets back...</td>\n","    </tr>\n","    <tr>\n","      <th>1234363</th>\n","      <td>1</td>\n","      <td>is basking in the sun</td>\n","    </tr>\n","    <tr>\n","      <th>1191119</th>\n","      <td>1</td>\n","      <td>@VickiElam U should meet @StatiK99 cause he's ...</td>\n","    </tr>\n","    <tr>\n","      <th>898330</th>\n","      <td>1</td>\n","      <td>400th tweet!!  yay</td>\n","    </tr>\n","    <tr>\n","      <th>1183352</th>\n","      <td>1</td>\n","      <td>Guess who finally has a new Blackberry?</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         polarity                                               text\n","1308994         1  thinking about laying down. pretty sure maggie...\n","1156126         1  @JonasVideos hey, thanks for following us. Tha...\n","1360524         1                       Going to Lido's for dinner. \n","541612          0                  Going to romania. Pretty scared. \n","301220          0        Can't sleep  testing twitter from my mobile\n","505774          0  Just loves how @justinjeremy NEVER tweets back...\n","1234363         1                             is basking in the sun \n","1191119         1  @VickiElam U should meet @StatiK99 cause he's ...\n","898330          1                                 400th tweet!!  yay\n","1183352         1          Guess who finally has a new Blackberry?  "]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["data.head(10)"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"vI1xFqAZdkku","outputId":"d09a39d1-b8be-4879-e7ec-6409800f80d8","trusted":true},"outputs":[{"data":{"text/plain":["polarity    0.0\n","text        0.0\n","dtype: float64"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["#Checking if any null values present\n","(data.isnull().sum() / len(data))*100"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"ThC3QfwupuUs","trusted":true},"outputs":[],"source":["#convrting pandas object to a string\n","data['text'] = data['text'].astype('str')"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"sUxte8kadkkt","outputId":"20ab1d84-f414-4935-d556-1a100f9c91f3","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{\"mightn't\", \"you're\", 'y', 'weren', 'didn', 'shouldn', \"doesn't\", 'here', 'your', 'yours', 'into', 'no', 'our', 'any', 'over', 'with', 'when', 'themselves', \"needn't\", 'these', \"don't\", 'shan', 'during', 'i', 'those', 'most', 'her', 'all', 'or', 'doing', 'we', 'as', \"she's\", 's', 'ours', 've', \"should've\", 'while', 'of', 'through', 'ma', 'up', 'isn', 'from', 'again', 'are', 'am', 'they', 'until', 'hers', \"haven't\", 'more', 'because', \"you've\", \"shouldn't\", 'him', 'what', 'you', 'both', 'them', 'only', 'very', 'who', 'will', \"isn't\", 'each', 'has', \"hasn't\", 'too', 'this', 'in', 'just', 'then', 'should', 'd', 'did', 'itself', 'out', 'me', 'aren', 'that', 'after', 'yourself', 'off', 'same', 're', 'own', 'ain', 'for', 'before', 'myself', 'some', 'whom', 'on', 'above', 'which', 'he', 'were', 'why', 'so', 'down', 'once', \"shan't\", \"it's\", 'o', 'hasn', 'do', 'couldn', \"you'll\", 'be', 'needn', 'an', 'other', 'don', 'was', \"mustn't\", \"wouldn't\", 'than', 'its', 'being', 'and', 'by', 'below', 'hadn', 'under', 'if', 'doesn', 'can', 'won', 'further', 'few', \"you'd\", \"aren't\", 'have', 'a', 'm', 'nor', 'between', 'himself', 'not', 'yourselves', 'but', 'at', \"won't\", 'having', \"that'll\", 'theirs', 'to', 'it', 'ourselves', 'haven', 'the', 'where', 'such', 't', 'been', 'his', \"hadn't\", 'is', 'there', 'mustn', 'she', 'herself', \"didn't\", 'wouldn', 'how', \"couldn't\", 'their', 'against', 'my', 'mightn', 'wasn', 'now', 'does', 'about', \"weren't\", 'll', 'had', \"wasn't\"}\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["nltk.download('stopwords')\n","stopword = set(stopwords.words('english'))\n","print(stopword)"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"eIbgnD6_p8Dr","outputId":"91ef58df-bf04-48cf-e7d9-2d2d2f48fec6","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('punkt')\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"1qK7KlBpdkkt","trusted":true},"outputs":[],"source":["urlPattern = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n","userPattern = '@[^\\s]+'\n","def process_tweets(tweet):\n","  # Lower Casing\n","    tweet = tweet.lower()\n","    tweet=tweet[1:]\n","    # Removing all URls \n","    tweet = re.sub(urlPattern,'',tweet)\n","    # Removing all @username.\n","    tweet = re.sub(userPattern,'', tweet) \n","    #Remove punctuations\n","    tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n","    #tokenizing words\n","    tokens = word_tokenize(tweet)\n","    #Removing Stop Words\n","    final_tokens = [w for w in tokens if w not in stopword]\n","    #reducing a word to its word stem \n","    wordLemm = WordNetLemmatizer()\n","    finalwords=[]\n","    for w in final_tokens:\n","      if len(w)>1:\n","        word = wordLemm.lemmatize(w)\n","        finalwords.append(word)\n","    return ' '.join(finalwords)"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"25A79MOtdkkv","outputId":"03c1e106-a052-44f0-df3b-7826f8044a68","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>polarity</th>\n","      <th>text</th>\n","      <th>processed_tweets</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>97918</th>\n","      <td>0</td>\n","      <td>@danecook really wanted to go to that show. co...</td>\n","      <td>danecook really wanted go show couldnt afford ...</td>\n","    </tr>\n","    <tr>\n","      <th>406827</th>\n","      <td>0</td>\n","      <td>Great. The power is off. And I have wet hair t...</td>\n","      <td>reat power wet hair need dried date night boy</td>\n","    </tr>\n","    <tr>\n","      <th>646821</th>\n","      <td>0</td>\n","      <td>@Orchidflower yes it would be - she'd taken th...</td>\n","      <td>orchidflower yes would shed taken dog walk amp...</td>\n","    </tr>\n","    <tr>\n","      <th>1060028</th>\n","      <td>1</td>\n","      <td>@trixr4kedzz they are cool bags and @dancingsh...</td>\n","      <td>trixr4kedzz cool bag friend great bag great se...</td>\n","    </tr>\n","    <tr>\n","      <th>1236164</th>\n","      <td>1</td>\n","      <td>@A_Phelly then I'll follow you into the dark  ...</td>\n","      <td>aphelly ill follow dark im talented finishing ...</td>\n","    </tr>\n","    <tr>\n","      <th>408233</th>\n","      <td>0</td>\n","      <td>is at my cuzin Amber's grad party but still a ...</td>\n","      <td>cuzin amber grad party still little upset morning</td>\n","    </tr>\n","    <tr>\n","      <th>1075008</th>\n","      <td>1</td>\n","      <td>@florinda_3rs Lucky @trishheylady. *stomping f...</td>\n","      <td>florinda3rs lucky stomping foot blogger fun be...</td>\n","    </tr>\n","    <tr>\n","      <th>463545</th>\n","      <td>0</td>\n","      <td>@AndrePaular that's awesome. I would donate bu...</td>\n","      <td>andrepaular thats awesome would donate havent ...</td>\n","    </tr>\n","    <tr>\n","      <th>747388</th>\n","      <td>0</td>\n","      <td>Alright so the Taste of Chicago starts this co...</td>\n","      <td>lright taste chicago start coming weekendif pl...</td>\n","    </tr>\n","    <tr>\n","      <th>334409</th>\n","      <td>0</td>\n","      <td>People come on..why do you keep deleting then ...</td>\n","      <td>eople come onwhy keep deleting refollowing im ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         polarity                                               text  \\\n","97918           0  @danecook really wanted to go to that show. co...   \n","406827          0  Great. The power is off. And I have wet hair t...   \n","646821          0  @Orchidflower yes it would be - she'd taken th...   \n","1060028         1  @trixr4kedzz they are cool bags and @dancingsh...   \n","1236164         1  @A_Phelly then I'll follow you into the dark  ...   \n","408233          0  is at my cuzin Amber's grad party but still a ...   \n","1075008         1  @florinda_3rs Lucky @trishheylady. *stomping f...   \n","463545          0  @AndrePaular that's awesome. I would donate bu...   \n","747388          0  Alright so the Taste of Chicago starts this co...   \n","334409          0  People come on..why do you keep deleting then ...   \n","\n","                                          processed_tweets  \n","97918    danecook really wanted go show couldnt afford ...  \n","406827       reat power wet hair need dried date night boy  \n","646821   orchidflower yes would shed taken dog walk amp...  \n","1060028  trixr4kedzz cool bag friend great bag great se...  \n","1236164  aphelly ill follow dark im talented finishing ...  \n","408233   cuzin amber grad party still little upset morning  \n","1075008  florinda3rs lucky stomping foot blogger fun be...  \n","463545   andrepaular thats awesome would donate havent ...  \n","747388   lright taste chicago start coming weekendif pl...  \n","334409   eople come onwhy keep deleting refollowing im ...  "]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["data.head(10)"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"z-O59gYEyV7B","trusted":true},"outputs":[],"source":["from keras.models import Sequential\n","from keras.layers import Dense, Embedding, LSTM, Dropout\n","from sklearn.feature_extraction.text import CountVectorizer\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.utils.np_utils import to_categorical\n","import re"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"_HpLbl0z-kZg","outputId":"7e4f396f-71b0-4af5-c2d5-fe4efa6e1d33","trusted":true},"outputs":[{"data":{"text/plain":["'2.10.0'"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["import keras\n","keras.__version__"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"68irxgj85Duv","outputId":"9fbd6393-c152-45f0-b4b6-63014e90ab70","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[[   0    0    0 ...  590  156  114]\n"," [   0    0    0 ...  593   22  246]\n"," [   0    0    0 ...   26 1743   91]\n"," ...\n"," [   0    0    0 ...   64   25  440]\n"," [   0    0    0 ...  229   22 2572]\n"," [   0    0    0 ... 1513  136   62]]\n"]}],"source":["from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from keras import regularizers\n","\n","max_words = 5000\n","max_len = 200\n","#tokenizer of keras\n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(data.processed_tweets)\n","sequences = tokenizer.texts_to_sequences(data.processed_tweets)\n","tweets = pad_sequences(sequences, maxlen=max_len)\n","print(tweets)"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"GVXtFMC453Hi","trusted":true},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(tweets, data.polarity.values, test_size=0.2, random_state=101)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Adding LSTM to the process input allows RNN to remember its inputs for a long time, which is useful for learning long-term dependencies. In this case, the word embedding is used to generate a numerical vector representation of each word, which is then fed into the LSTM layer. Weâ€™ve used keras tokeniser and pad sequences to turn words into tokens and sequences, which we then used to generate word embeddings. We have used three activation functions as hidden layers. Two of them are Relu, and the last one is Sigmoid activation function."]},{"cell_type":"code","execution_count":43,"metadata":{"id":"eGpuqkUj5Dxv","outputId":"07dc0998-3e41-412a-a49b-e0ec9fcfd3ef","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/10\n","5000/5000 [==============================] - ETA: 0s - loss: 0.5066 - accuracy: 0.7479\n","Epoch 1: val_accuracy improved from -inf to 0.76998, saving model to rnn_model.hdf5\n","5000/5000 [==============================] - 915s 182ms/step - loss: 0.5066 - accuracy: 0.7479 - val_loss: 0.4771 - val_accuracy: 0.7700\n","Epoch 2/10\n","5000/5000 [==============================] - ETA: 0s - loss: 0.4672 - accuracy: 0.7749\n","Epoch 2: val_accuracy improved from 0.76998 to 0.77363, saving model to rnn_model.hdf5\n","5000/5000 [==============================] - 960s 192ms/step - loss: 0.4672 - accuracy: 0.7749 - val_loss: 0.4715 - val_accuracy: 0.7736\n","Epoch 3/10\n","5000/5000 [==============================] - ETA: 0s - loss: 0.4518 - accuracy: 0.7832\n","Epoch 3: val_accuracy improved from 0.77363 to 0.77387, saving model to rnn_model.hdf5\n","5000/5000 [==============================] - 699s 140ms/step - loss: 0.4518 - accuracy: 0.7832 - val_loss: 0.4706 - val_accuracy: 0.7739\n","Epoch 4/10\n","5000/5000 [==============================] - ETA: 0s - loss: 0.4391 - accuracy: 0.7914\n","Epoch 4: val_accuracy did not improve from 0.77387\n","5000/5000 [==============================] - 737s 147ms/step - loss: 0.4391 - accuracy: 0.7914 - val_loss: 0.4790 - val_accuracy: 0.7717\n","Epoch 5/10\n","5000/5000 [==============================] - ETA: 0s - loss: 0.4270 - accuracy: 0.7975\n","Epoch 5: val_accuracy did not improve from 0.77387\n","5000/5000 [==============================] - 700s 140ms/step - loss: 0.4270 - accuracy: 0.7975 - val_loss: 0.4835 - val_accuracy: 0.7709\n","Epoch 6/10\n","5000/5000 [==============================] - ETA: 0s - loss: 0.4150 - accuracy: 0.8038\n","Epoch 6: val_accuracy did not improve from 0.77387\n","5000/5000 [==============================] - 702s 140ms/step - loss: 0.4150 - accuracy: 0.8038 - val_loss: 0.4884 - val_accuracy: 0.7721\n","Epoch 7/10\n","5000/5000 [==============================] - ETA: 0s - loss: 0.4034 - accuracy: 0.8110\n","Epoch 7: val_accuracy did not improve from 0.77387\n","5000/5000 [==============================] - 1029s 206ms/step - loss: 0.4034 - accuracy: 0.8110 - val_loss: 0.4918 - val_accuracy: 0.7714\n","Epoch 8/10\n","5000/5000 [==============================] - ETA: 0s - loss: 0.3931 - accuracy: 0.8164\n","Epoch 8: val_accuracy did not improve from 0.77387\n","5000/5000 [==============================] - 1115s 223ms/step - loss: 0.3931 - accuracy: 0.8164 - val_loss: 0.5010 - val_accuracy: 0.7670\n","Epoch 9/10\n","5000/5000 [==============================] - ETA: 0s - loss: 0.3822 - accuracy: 0.8224\n","Epoch 9: val_accuracy did not improve from 0.77387\n","5000/5000 [==============================] - 888s 178ms/step - loss: 0.3822 - accuracy: 0.8224 - val_loss: 0.5174 - val_accuracy: 0.7659\n","Epoch 10/10\n","5000/5000 [==============================] - ETA: 0s - loss: 0.3748 - accuracy: 0.8260\n","Epoch 10: val_accuracy did not improve from 0.77387\n","5000/5000 [==============================] - 818s 164ms/step - loss: 0.3748 - accuracy: 0.8260 - val_loss: 0.5277 - val_accuracy: 0.7627\n"]}],"source":["from keras.models import Sequential\n","from keras import layers\n","from keras import regularizers\n","from keras import backend as K\n","from keras.callbacks import ModelCheckpoint\n","model2 = Sequential()\n","model2.add(layers.Embedding(max_words, 128))\n","model2.add(layers.LSTM(64,dropout=0.5))\n","model2.add(layers.Dense(16, activation='relu'))\n","model2.add(layers.Dense(8, activation='relu'))\n","model2.add(layers.Dense(1,activation='sigmoid'))\n","model2.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\n","checkpoint2 = ModelCheckpoint(\"rnn_model.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\n","history = model2.fit(X_train, y_train, epochs=10,validation_data=(X_test, y_test),callbacks=[checkpoint2])\n"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"NjU0Kf7idfJf","outputId":"ed0c9408-007f-4473-e6a3-fe70647507e8","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 331ms/step\n","Positive\n"]}],"source":["sequence = tokenizer.texts_to_sequences(['today is my birhtday. had a good day.'])\n","test = pad_sequences(sequence, maxlen=max_len)\n","pred = model2.predict(test)\n","if pred > 0.5:\n","  print('Positive')\n","else:\n","  print('Negative')\n","# print(pred)"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"HJXJWC53PGEp","outputId":"539028f1-fcf9-443e-a8b3-be82b43a78ef","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 1s 682ms/step\n","Positive\n"]}],"source":["model = keras.models.load_model('rnn_model.hdf5')\n","sequence = tokenizer.texts_to_sequences([' the day is very good'])\n","test = pad_sequences(sequence, maxlen=max_len)\n","pred = model.predict(test)\n","if pred > 0.5:\n","  print('Positive')\n","else:\n","  print('Negative')"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"_Bw4rZQjTWEw","outputId":"93705aa3-e7bc-49a3-d356-84bdecaeec84","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 114ms/step\n","Negative\n"]}],"source":["sequence = tokenizer.texts_to_sequences(['I am feeling bad today'])\n","test = pad_sequences(sequence, maxlen=max_len)\n","pred = model.predict(test)\n","if pred > 0.5:\n","  print('Positive')\n","else:\n","  print('Negative')"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.5 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"27f6fea6f47ae512550f0b8facdbd035a93e1dd89633f7bf2dd00a2502c71d0d"}}},"nbformat":4,"nbformat_minor":4}
